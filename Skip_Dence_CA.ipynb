{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Skip_Dence_CA.ipynb","provenance":[],"collapsed_sections":["JuIOHKi9XO3F"],"toc_visible":true,"authorship_tag":"ABX9TyP87RboueLxxYn2DY9H8Iq8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"be825b7fe9f04a5aae7e587fb9de3b00":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_596e97bff0be4720bd7ccde0c816cb5e","IPY_MODEL_9952953ea2d7494aab58d57755a93fca","IPY_MODEL_d6ef7d7fe98a49d3b94edf57c6d037ba"],"layout":"IPY_MODEL_68cbc7092a5c4be7af613f3c3bbfabe9"}},"596e97bff0be4720bd7ccde0c816cb5e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_03dbe005edf945d095f7ef33d4aa5c25","placeholder":"​","style":"IPY_MODEL_036f82d5083144debdc1a3a521f06923","value":""}},"9952953ea2d7494aab58d57755a93fca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a92566be17734e1fb5574dbe204dcd3c","max":170498071,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5a9b7a70db5d4db6ac18cffc06b2e45a","value":170498071}},"d6ef7d7fe98a49d3b94edf57c6d037ba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ccf6d6a9d5454dfb9af1adba72e6b887","placeholder":"​","style":"IPY_MODEL_c56ff46957894921ba1e0c8563b48d63","value":" 170499072/? [00:01&lt;00:00, 92197195.60it/s]"}},"68cbc7092a5c4be7af613f3c3bbfabe9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03dbe005edf945d095f7ef33d4aa5c25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"036f82d5083144debdc1a3a521f06923":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a92566be17734e1fb5574dbe204dcd3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a9b7a70db5d4db6ac18cffc06b2e45a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ccf6d6a9d5454dfb9af1adba72e6b887":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c56ff46957894921ba1e0c8563b48d63":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# CIFAR-10 데이터셋 분류(Skip Conncection, Dense Connection, Channel Attention)"],"metadata":{"id":"xSUVndE6W5O2"}},{"cell_type":"markdown","source":["### 폴더 경로 저장"],"metadata":{"id":"byAFT8lVXGhu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qNHTrB-HDlkg"},"outputs":[],"source":["datasetPath = \"./drive/MyDrive/dataset/\"\n","parameterPath = \"./drive/MyDrive/parameters/\""]},{"cell_type":"markdown","source":["### 패키지 선언"],"metadata":{"id":"kwwdzC25XJwW"}},{"cell_type":"code","source":["\n","import torch\n","import torch.nn as nn\n","import torchvision.datasets as dataset\n","import torchvision.transforms as transform\n","from torch.utils.data import DataLoader\n","import numpy as np"],"metadata":{"id":"1v-fDXqyDusA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 데이터셋 선언 (CIFAR10)"],"metadata":{"id":"JuIOHKi9XO3F"}},{"cell_type":"code","source":["# Training dataset 다운로드\n","cifar10_train = dataset.CIFAR10(root = datasetPath, # 데이터셋이 저장된 위치\n","                            train = True,\n","                            transform = transform.ToTensor(),\n","                            download = True)\n","# Testing dataset 다운로드\n","cifar10_test = dataset.CIFAR10(root = datasetPath,\n","                            train = False,\n","                            transform = transform.ToTensor(),\n","                            download = True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":104,"referenced_widgets":["be825b7fe9f04a5aae7e587fb9de3b00","596e97bff0be4720bd7ccde0c816cb5e","9952953ea2d7494aab58d57755a93fca","d6ef7d7fe98a49d3b94edf57c6d037ba","68cbc7092a5c4be7af613f3c3bbfabe9","03dbe005edf945d095f7ef33d4aa5c25","036f82d5083144debdc1a3a521f06923","a92566be17734e1fb5574dbe204dcd3c","5a9b7a70db5d4db6ac18cffc06b2e45a","ccf6d6a9d5454dfb9af1adba72e6b887","c56ff46957894921ba1e0c8563b48d63"]},"id":"tiruEWf0DztS","executionInfo":{"status":"ok","timestamp":1653280154548,"user_tz":-540,"elapsed":7081,"user":{"displayName":"백수민","userId":"11277539835389427967"}},"outputId":"f25c4514-d20e-47ea-ae08-e5e1665b5c81"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./drive/MyDrive/dataset/cifar-10-python.tar.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be825b7fe9f04a5aae7e587fb9de3b00"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./drive/MyDrive/dataset/cifar-10-python.tar.gz to ./drive/MyDrive/dataset/\n","Files already downloaded and verified\n"]}]},{"cell_type":"markdown","source":["## Skip Connection"],"metadata":{"id":"hCsE_A6RXSlu"}},{"cell_type":"code","source":["#CNN 모델 (VGG) 정의\n","class VGG_SKIP (nn.Module):\n","    def __init__(self): # 신경망 구성요소 정의\n","        super(VGG_SKIP, self).__init__()\n","        self.conv1_1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n","        self.conv1_2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n","               \n","        self.conv2_1 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n","        self.conv2_2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n","        \n","        self.conv3_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n","        self.conv3_2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n","\n","        # Skip Connection을 위한 Conv, layer\n","        self.conv_skip1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n","        self.conv_skip2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n","        self.conv_skip3 = nn.Conv2d(in_channels=64, out_channels=256, kernel_size=3, padding=1)\n","\n","        self.fc1 = nn.Linear(in_features=4096, out_features=512)\n","        self.fc2 = nn.Linear(in_features=512, out_features=256)\n","        self.fc3 = nn.Linear(in_features=256, out_features=10)\n","        \n","        self.relu = nn.ReLU()\n","        self.avgPool2d = nn.AvgPool2d(kernel_size=2, stride=2)\n","\n","    def forward(self,x):\n","\n","        input_feature1 = x  # skip 입력을 위한 input 저장\n","        out = self.relu(self.conv1_1(x))\n","        out = self.relu(self.conv1_2(out))\n","        input_skip1 = self.relu(self.conv_skip1(input_feature1))  # skip 입력을 위한 con layer 적용\n","        out = torch.add(out, input_skip1) # skip connection 적용\n","        out = self.avgPool2d(out)\n","\n","        input_feature2 = out  # skip 입력을 위한 input 저장\n","        out = self.relu(self.conv2_1(out))\n","        out = self.relu(self.conv2_2(out))\n","        input_skip2 = self.relu(self.conv_skip2(input_feature2))  # skip 입력을 위한 con layer 적용\n","        out = torch.add(out, input_skip2) # skip connection 적용\n","        out = self.avgPool2d(out)\n","\n","        input_feature3 = out  # skip 입력을 위한 input 저장\n","        out = self.relu(self.conv3_1(out))\n","        out = self.relu(self.conv3_2(out))\n","        input_skip3 = self.relu(self.conv_skip3(input_feature1))  # skip 입력을 위한 con layer 적용\n","        out = torch.add(out, input_skip3) # skip connection 적용\n","        out = self.avgPool2d(out)\n","\n","        out = out.view(-1, 4096) # feature map 평탄화\n","\n","        out = self.relu(self.fc1(out))\n","        out = self.relu(self.fc2(out))\n","        out = self.fc3(out)\n","        return out"],"metadata":{"id":"Kqly1L8uD2KH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Hyper-parameters 지정\n","training_epoch = 20\n","batch_size = 100\n","learning_rate = 0.1\n","loss_function = nn.CrossEntropyLoss()\n","network = VGG_SKIP()\n","optimizer = torch.optim.SGD(network.parameters(), lr = learning_rate)\n","\n","data_loader = DataLoader(dataset=cifar10_train,\n","                         batch_size=batch_size,\n","                         shuffle=True,\n","                         drop_last=True)"],"metadata":{"id":"xka5vPpgD4v4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Trainin loop w/ GPU\n","#- Colab 상단 런타임 - 런타임 유형 변경 - 하드웨어 가속기 에서 GPU 선택 - 저장\n","#- !nvidia-smi 명령어로 GPU 할당 여부 확인\n","#- network parameter 및 data 유형을 cuda:0으로 설정\n","network.train()\n","network = network.to('cuda:0')\n","for epoch in range(training_epoch):\n","    avg_cost = 0\n","    total_batch = len(data_loader)\n","\n","    for img, label in data_loader:\n","        \n","        img = img.to('cuda:0')\n","        label = label.to('cuda:0')\n","        pred = network(img)\n","        loss = loss_function(pred, label)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        avg_cost += loss / total_batch\n","\n","    print('Epoch: %d Loss = %f'%(epoch+1, avg_cost))\n","\n","print('Learning finished')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":418},"id":"upvFZKROD7yG","executionInfo":{"status":"error","timestamp":1653280166686,"user_tz":-540,"elapsed":11529,"user":{"displayName":"백수민","userId":"11277539835389427967"}},"outputId":"ac9a97bc-dfe3-474f-fffb-86429cc3c272"},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-250b724132b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-abae45fbf5bf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0minput_skip3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_skip3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_feature1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# skip 입력을 위한 con layer 적용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_skip3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# skip connection 적용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgPool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    442\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    443\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 444\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [256, 64, 3, 3], expected input[100, 3, 32, 32] to have 64 channels, but got 3 channels instead"]}]},{"cell_type":"code","source":["#Weight parameter 저장\n","torch.save(network.state_dict(), parameterPath+\"vgg_cifar10.pth\")"],"metadata":{"id":"PoegDP4LD_M7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#분류 성능 확인\n","network.eval()\n","network = network.to('cpu')\n","img_test = torch.tensor(np.transpose(cifar10_test.data , (0, 3, 1, 2))) /255.\n","label_test = torch.tensor(cifar10_test.targets)\n","\n","with torch.no_grad(): # test에서는 기울기 계산 제외\n","    prediction = network(img_test) # 전체 test data를 한번에 계산\n","\n","correct_prediction = torch.argmax(prediction, 1) == label_test\n","accuracy = correct_prediction.float().mean()\n","print('Accuracy:', accuracy.item())"],"metadata":{"id":"NYQgSlVOEBFf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"MngFfEWTEBgi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dence Connection"],"metadata":{"id":"Bou_M6PsXzpU"}},{"cell_type":"code","source":["#CNN 모델 (VGG) 정의\n","class VGG_DENSE (nn.Module):\n","    def __init__(self): # 신경망 구성요소 정의\n","        super(VGG_DENSE, self).__init__()\n","        self.conv1_1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n","        self.conv1_2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n","               \n","        self.conv2_1 = nn.Conv2d(in_channels=35, out_channels=32, kernel_size=3, padding=1)\n","        self.conv2_2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n","        \n","        self.conv3_1 = nn.Conv2d(in_channels=99, out_channels=128, kernel_size=3, padding=1)\n","        self.conv3_2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n","        \n","        self.fc1 = nn.Linear(in_features=4096, out_features=512)\n","        self.fc2 = nn.Linear(in_features=512, out_features=256)\n","        self.fc3 = nn.Linear(in_features=256, out_features=10)\n","        \n","        self.relu = nn.ReLU()\n","        self.avgPool2d = nn.AvgPool2d(kernel_size=2, stride=2)\n","\n","    def forward(self,x):\n","\n","        out1 = self.relu(self.conv1_1(x))\n","        out1 = self.relu(self.conv1_2(out1))\n","        out1 = torch.cat([x, out1], dim=1)\n","        out1 = self.avgPool2d(out1)\n","\n","        out2 = self.relu(self.conv2_1(out1))\n","        out2 = self.relu(self.conv2_2(out2))\n","        out2 = torch.cat([out1, out2], dim=1)\n","        out2 = self.avgPool2d(out2)\n","\n","        out3 = self.relu(self.conv3_1(out2))\n","        out3 = self.relu(self.conv3_2(out3))\n","        #out3 = torch.cat([out2, out3], dim=1)\n","        out = self.avgPool2d(out3)\n","\n","        out = out.view(-1, 4096) # feature map 평탄화\n","\n","        out = self.relu(self.fc1(out))\n","        out = self.relu(self.fc2(out))\n","        out = self.fc3(out)\n","        return out"],"metadata":{"id":"4rdewuE8X6EP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Trainin loop w/ GPU\n","#- Colab 상단 런타임 - 런타임 유형 변경 - 하드웨어 가속기 에서 GPU 선택 - 저장\n","#- !nvidia-smi 명령어로 GPU 할당 여부 확인\n","#- network parameter 및 data 유형을 cuda:0으로 설정\n","network.train()\n","network = network.to('cuda:0')\n","for epoch in range(training_epoch):\n","    avg_cost = 0\n","    total_batch = len(data_loader)\n","\n","    for img, label in data_loader:\n","        \n","        img = img.to('cuda:0')\n","        label = label.to('cuda:0')\n","        pred = network(img)\n","        loss = loss_function(pred, label)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        avg_cost += loss / total_batch\n","\n","    print('Epoch: %d Loss = %f'%(epoch+1, avg_cost))\n","\n","print('Learning finished')"],"metadata":{"id":"SkuhFddTX9im"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Weight parameter 저장\n","torch.save(network.state_dict(), parameterPath+\"vgg_cifar10.pth"],"metadata":{"id":"MQx46qVRX_wv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#분류 성능 확인\n","network.eval()\n","network = network.to('cpu')\n","img_test = torch.tensor(np.transpose(cifar10_test.data , (0, 3, 1, 2))) /255.\n","label_test = torch.tensor(cifar10_test.targets)\n","\n","with torch.no_grad(): # test에서는 기울기 계산 제외\n","    prediction = network(img_test) # 전체 test data를 한번에 계산\n","\n","correct_prediction = torch.argmax(prediction, 1) == label_test\n","accuracy = correct_prediction.float().mean()\n","print('Accuracy:', accuracy.item())"],"metadata":{"id":"lL991gFDYBmO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Channel_Attention"],"metadata":{"id":"e7oeYh9JYWjX"}},{"cell_type":"code","source":["#CNN 모델 (VGG) 정의\n","class VGG_CA (nn.Module):\n","    def __init__(self): # 신경망 구성요소 정의\n","        super(VGG_CA, self).__init__()\n","        self.conv1_1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n","        self.conv1_2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n","               \n","        self.conv2_1 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n","        self.conv2_2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n","        \n","        self.conv3_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n","        self.conv3_2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n","        \n","        self.fc1 = nn.Linear(in_features=4096, out_features=512)\n","        self.fc2 = nn.Linear(in_features=512, out_features=256)\n","        self.fc3 = nn.Linear(in_features=256, out_features=10)\n","\n","\n","        # Channel Attention\n","        self.adaptiveAvgPool2d = nn.AdaptiveAvgPool2d((1, 1)) # Global average pooling\n","        self.caconv1 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=1)\n","        self.caconv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","        self.relu = nn.ReLU()\n","        self.avgPool2d = nn.AvgPool2d(kernel_size=2, stride=2)\n","\n","    def forward(self,x):\n","\n","        out = self.relu(self.conv1_1(x))\n","        out = self.relu(self.conv1_2(out))\n","        out = self.avgPool2d(out)\n","\n","        out = self.relu(self.conv2_1(out))\n","        out = self.relu(self.conv2_2(out))\n","        \n","        ## Channel attention 적용\n","        caout = self.adaptiveAvgPool2d(out)\n","        caout = self.relu(self.caconv1(caout))\n","        caout = self.sigmoid(self.caconv2(caout))\n","        CA_map = caout.expand_as(out)\n","        out = out * CA_map\n","        \n","        out = self.avgPool2d(out)\n","\n","        out = self.relu(self.conv3_1(out))\n","        out = self.relu(self.conv3_2(out))\n","        out = self.avgPool2d(out)\n","\n","        out = out.view(-1, 4096) # feature map 평탄화\n","\n","        out = self.relu(self.fc1(out))\n","        out = self.relu(self.fc2(out))\n","        out = self.fc3(out)\n","        return out"],"metadata":{"id":"gOGVsNowYZw2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Trainin loop w/ GPU\n","#- Colab 상단 런타임 - 런타임 유형 변경 - 하드웨어 가속기 에서 GPU 선택 - 저장\n","#- !nvidia-smi 명령어로 GPU 할당 여부 확인\n","#- network parameter 및 data 유형을 cuda:0으로 설정\n","network.train()\n","network = network.to('cuda:0')\n","for epoch in range(training_epoch):\n","    avg_cost = 0\n","    total_batch = len(data_loader)\n","\n","    for img, label in data_loader:\n","        \n","        img = img.to('cuda:0')\n","        label = label.to('cuda:0')\n","        pred = network(img)\n","        loss = loss_function(pred, label)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        avg_cost += loss / total_batch\n","\n","    print('Epoch: %d Loss = %f'%(epoch+1, avg_cost))\n","\n","print('Learning finished')"],"metadata":{"id":"jBYjx_A6Yc_e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Weight parameter 저장\n","torch.save(network.state_dict(), parameterPath+\"vgg_cifar10.pth\")\n"],"metadata":{"id":"XTWfJxtZYe1m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#분류 성능 확인\n","network.eval()\n","network = network.to('cpu')\n","img_test = torch.tensor(np.transpose(cifar10_test.data , (0, 3, 1, 2))) /255.\n","label_test = torch.tensor(cifar10_test.targets)\n","\n","with torch.no_grad(): # test에서는 기울기 계산 제외\n","    prediction = network(img_test) # 전체 test data를 한번에 계산\n","\n","correct_prediction = torch.argmax(prediction, 1) == label_test\n","accuracy = correct_prediction.float().mean()\n","print('Accuracy:', accuracy.item())"],"metadata":{"id":"eDKpfKE8YgOe"},"execution_count":null,"outputs":[]}]}